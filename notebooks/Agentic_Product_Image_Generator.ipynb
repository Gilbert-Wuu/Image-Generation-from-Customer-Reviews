{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install -r requirements.txt\n",
        "!pip install ipykernel langchain_experimental llama-index-vector-stores-pinecone ipykernel PyMuPDF pinecone-client pypdf faiss-cpu langchain_community transformers sentence_transformers\n",
        "!pip install llama_index.embeddings.huggingface\n",
        "!pip install transformers pinecone-client pypdf faiss-cpu langchain langchain-openai langchain-community\n",
        "!pip install torch sentence-transformers scikit-learn pandas numpy nltk openai python-dotenv llama-index pymupdf\n",
        "!pip install huggingface_hub datasets IProgress"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TEg4z3KVxE7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import sqlite3\n",
        "import time\n",
        "\n",
        "import datasets\n",
        "import dotenv\n",
        "import faiss\n",
        "import fitz\n",
        "import huggingface_hub\n",
        "import langchain\n",
        "import langchain_community\n",
        "import nltk\n",
        "import numpy as np\n",
        "import openai\n",
        "import pandas as pd\n",
        "import pinecone\n",
        "import pypdf\n",
        "import requests\n",
        "import torch\n",
        "import transformers\n",
        "from dotenv import load_dotenv\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from llama_index.core import (SimpleDirectoryReader, StorageContext,\n",
        "                              VectorStoreIndex)\n",
        "from llama_index.core.extractors import (QuestionsAnsweredExtractor,\n",
        "                                         TitleExtractor)\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "from llama_index.core.schema import TextNode\n",
        "# sentence transformers\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "2M3OuYywxEXE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CnfWNjm2tHr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chunk text and store in pinecone db - Run Once\n",
        "load_dotenv()\n",
        "\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "pinecone_api_key=userdata.get('PINECONE_KEY')\n",
        "\n",
        "# configure Pinecone client\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "PDF_PATH = \"/content/drive/MyDrive/GenAI Image Generation/Product_review.pdf\"\n",
        "doc = fitz.open(PDF_PATH)\n",
        "\n",
        "\n",
        "# # parser to split up product review:\n",
        "text_parser = TokenTextSplitter(\n",
        "    chunk_size=1024\n",
        ")\n",
        "\n",
        "text_chunks = []\n",
        "doc_idxs = []\n",
        "\n",
        "\n",
        "for doc_idx, page in enumerate(doc):\n",
        "    page_text = page.get_text(\"text\")\n",
        "    cur_text_chunks = text_parser.split_text(page_text)\n",
        "    text_chunks.extend(cur_text_chunks)\n",
        "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
        "\n",
        "nodes = []\n",
        "\n",
        "for idx, text_chunk in enumerate(text_chunks):\n",
        "    node = TextNode(\n",
        "        text=text_chunk,\n",
        "    )\n",
        "    src_doc_idx = doc_idxs[idx]\n",
        "    src_page = doc[src_doc_idx]\n",
        "    nodes.append(node)\n",
        "\n",
        "use_serverless = True\n",
        "\n",
        "spec=ServerlessSpec(\n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\"\n",
        "    )\n",
        "\n",
        "# specify the Pinecone environment to use:\n",
        "if use_serverless:\n",
        "    spec = pinecone.ServerlessSpec(cloud='aws', region=\"us-east-1\")\n",
        "else:\n",
        "   spec = pinecone.PodSpec(environment=environment)\n",
        "\n",
        "# Name our Pinecone Index:\n",
        "index_name = \"hw04\"\n",
        "\n",
        "# If a Pinecone index of the same name already exists, delete it:\n",
        "if index_name in pc.list_indexes().names():\n",
        "    pc.delete_index(index_name)\n",
        "\n",
        "\n",
        "# define similarity and additional parameters for the vector store index:\n",
        "dimensions = 1536              # the dimensions of the index need to align with the LLM we are using for the RAG system. For example, if using openAI then dimenion = 1536. If using Llama2, then dimension = 384.\n",
        "\n",
        "# \"dotproduct\" is one similarity metric we can for the vector store index. We can use different distance metrics to measure the similarity between vector embeddings and user queries. This is where we define what similarity metric we are going to use for the vector store.\n",
        "# \"cosine\" is another similarity metric we can use for the vector store index.\n",
        "# \"euclidean\" is another similarity metric we can use for the vector store index.\n",
        "\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=dimensions,\n",
        "    metric=\"cosine\",          # we can use different distance metrics to measure the similarity between vector embeddings and user queries. this is where we define what similarity metric we are going to use for the vector store.\n",
        "    spec=spec\n",
        ")\n",
        "\n",
        "# wait for index to be ready before connecting\n",
        "while not pc.describe_index(index_name).status['ready']:\n",
        "   time.sleep(1)\n",
        "\n",
        "for index in pc.list_indexes():\n",
        "    print(index['name'])\n",
        "\n",
        "\n",
        "pc.describe_index(\"hw04\")\n",
        "\n",
        "\n",
        "pc_index = pc.Index(index_name)  # create an index to use in the vector store\n",
        "\n",
        "\n",
        "vector_store = PineconeVectorStore(pinecone_index=pc_index)    # this function creates a vector store where we will add and store embeddings\n",
        "\n",
        "pc_index = pc.Index(index_name)  # create an index to use in the vector store\n",
        "vector_store = PineconeVectorStore(pinecone_index=pc_index)    # this function creates a vector store where we will add and store embeddings\n",
        "\n",
        "pc_index.describe_index_stats()\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\",\n",
        "             api_key=openai_api_key)\n",
        "\n",
        "extractors = [\n",
        "    TitleExtractor(nodes=5, llm=llm),\n",
        "    QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
        "]\n",
        "\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=extractors,\n",
        ")\n",
        "nodes = await pipeline.arun(nodes=nodes, in_place=False)\n",
        "\n",
        "model_ada=\"text-embedding-ada-002\"\n",
        "small_txt_embedmodel_=\"text-embedding-3-small\"\n",
        "\n",
        "\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", api_key=openai_api_key)\n",
        "\n",
        "for node in nodes:\n",
        "    node_embedding = embed_model.get_text_embedding(\n",
        "        node.get_content(metadata_mode=\"all\")\n",
        "    )\n",
        "    node.embedding = node_embedding\n",
        "\n",
        "vector_store.add(nodes)\n",
        "\n",
        "\n",
        "pc_index.describe_index_stats()\n",
        "\n",
        "\n",
        "print(nodes[0].metadata)\n",
        "\n",
        "print(nodes[0])"
      ],
      "metadata": {
        "id": "wzGXeXL7xUud",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to generate image prompt for product based on product name, returns prompt string\n",
        "\n",
        "def get_image_prompt_for_product(product_name: str) -> str:\n",
        "\n",
        "  client = openai.OpenAI(api_key=openai_api_key)\n",
        "\n",
        "  # query the vector store with the 5 queries above (don't forget to record the responses in your homework submission!)\n",
        "\n",
        "  k = 5\n",
        "  queries = [\n",
        "      f\"Summarize the product description and customer reviews in a concise bullet-point format highlighting the key features and overall user sentiment for {product_name}\",\n",
        "\n",
        "  ]\n",
        "  responses = []\n",
        "\n",
        "\n",
        "  # Choose one of these models:\n",
        "  embed_model_ada = \"text-embedding-ada-002\"\n",
        "  embed_model_3_small = \"text-embedding-3-small\"\n",
        "\n",
        "  for query in tqdm(queries):\n",
        "      res = client.embeddings.create(\n",
        "          input=[query],\n",
        "          model=embed_model_3_small\n",
        "      )\n",
        "\n",
        "      # Retrieve from Pinecone\n",
        "      xq = res.data[0].embedding  # res['data'][0]['embedding']\n",
        "\n",
        "      # Get relevant contexts (including the questions)\n",
        "      res2 = pc_index.query(vector=xq, top_k=k, include_metadata=True)\n",
        "\n",
        "      # Add response results\n",
        "      responses.append(res2)\n",
        "\n",
        "  chat_responses = []\n",
        "  for query, response in zip(queries, responses):\n",
        "      chat_response = client.chat.completions.create(\n",
        "      model=\"gpt-4o\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"Instruction: use the information in {response} to answer the user's question.\"},\n",
        "          {\"role\": \"user\", \"content\": query},\n",
        "          {\"role\": \"assistant\", \"content\": str(response)},\n",
        "          {\"role\": \"user\", \"content\": \"What is the answer?\"}\n",
        "      ]\n",
        "      )\n",
        "      chat_responses.append(chat_response.choices[0].message.content)\n",
        "\n",
        "  print(chat_responses[0])\n",
        "  return chat_responses[0]"
      ],
      "metadata": {
        "id": "s35OtXtAxeVj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example\n",
        "prom = get_image_prompt_for_product(\"bomber jacket\")\n",
        "print (prom)"
      ],
      "metadata": {
        "id": "uPmvPP8_091Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4UVzXXyJ9ejD"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "login(userdata.get('HF_KEY'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# Initialize the OpenAI client with your API key\n",
        "client = OpenAI(\n",
        "    api_key=userdata.get('OPENAI_API_KEY')\n",
        ")"
      ],
      "metadata": {
        "id": "pv1dSMy7EbDp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch"
      ],
      "metadata": {
        "id": "Xl15_bYp9qoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"Using GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")"
      ],
      "metadata": {
        "id": "c6I-94TK9uDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to clean prompt: remove spaces and special characters from prompt\n",
        "\n",
        "def create_prompt_image_generation(text: str) -> str:\n",
        "    # Replace newline characters with a space\n",
        "    no_newlines = text.replace('\\n', ' ')\n",
        "    # Remove inverted commas (both single and double quotes)\n",
        "    cleaned_text = no_newlines.replace('\"', '').replace(\"'\", \"\")\n",
        "    # Remove any extra spaces and return a single paragraph\n",
        "    single_paragraph = ' '.join(cleaned_text.split())\n",
        "    return single_paragraph\n"
      ],
      "metadata": {
        "id": "_a4a8e4a-AOI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "#Function to save Dalle image to Google drive path\n",
        "\n",
        "def save_image_from_url(image_url: str, save_path: str):\n",
        "    response = requests.get(image_url)\n",
        "    if response.status_code == 200:\n",
        "        with open(save_path, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Image successfully saved to {save_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to download image. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "Q_1Zj_hfkzNH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Stable Diffusion model (adjust torch_dtype and device as needed)\n",
        "\n",
        "model_id = \"Manojb/stable-diffusion-2-1-base\"\n",
        "\n",
        "print(f\"Loading {model_id}...\")\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "print(\"Stable Diffusion 2.1 Base loaded successfully and ready for fast generation.\")"
      ],
      "metadata": {
        "id": "TRIoiXBLmVtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image_sd(prompt_item: str, prompt_final: str, prompt_item_num: str):\n",
        "    \"\"\"\n",
        "    Generates images using Stable Diffusion (via the global 'pipe' object).\n",
        "\n",
        "    For Stable Diffusion:\n",
        "      - The image is generated using the diffusers pipeline.\n",
        "      - The image is saved locally with the filename format:\n",
        "        \"{prompt_item}_sd_{prompt_item_num}.png\"\n",
        "    \"\"\"\n",
        "    # --- Stable Diffusion Image Generation ---\n",
        "    cleaned_prompt_item = prompt_item.replace(\" \", \"_\")\n",
        "\n",
        "    print(\"Generating Stable Diffusion Image...\")\n",
        "\n",
        "    # Generate the image using the prompt_final\n",
        "    sd_image = pipe(\n",
        "        prompt_final,\n",
        "        num_inference_steps=25,\n",
        "        guidance_scale=8.0\n",
        "    ).images[0]\n",
        "\n",
        "    # Create the image name for stable diffusion\n",
        "    image_name_sd = \"/content/drive/MyDrive/GenAI Image Generation/image/\"+f\"{prompt_item}_sd_{prompt_item_num}.png\"\n",
        "    sd_image.save(image_name_sd)\n",
        "    print(f\"Stable Diffusion image saved as {image_name_sd}\")\n",
        "\n",
        "    return image_name_sd"
      ],
      "metadata": {
        "id": "-hXFmIHiHmEp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to generate image from Dalle based on prompt item name, prompt item number - image name parameter, final prompt text\n",
        "#Returns file path of saved image from Dalle\n",
        "\n",
        "def generate_image_dalle(prompt_item: str, prompt_final: str, prompt_item_num: str) -> str:\n",
        "    \"\"\"\n",
        "    Generates images using DALL-E based on the provided prompt.\n",
        "\n",
        "    For DALL-E:\n",
        "      - The image is generated using the DALL-E client.\n",
        "      - The URL of the generated image is printed.\n",
        "\n",
        "    Note: Ensure that the DALL-E client (referred to as 'client') is properly initialized before calling this function.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    dalle_prompt=\"The image should be clean and professional, with no text, words, labels, or reviews included in the image. \"+prompt_final\n",
        "\n",
        "    # --- DALL-E Image Generation ---\n",
        "    # Ensure that 'client' is initialized for DALL-E image generation.\n",
        "    response = client.images.generate(\n",
        "        model=\"dall-e-3\",\n",
        "        prompt=dalle_prompt,\n",
        "        size='1024x1024',\n",
        "        style='vivid',\n",
        "        quality='hd',\n",
        "        n=1\n",
        "    )\n",
        "    image_url = response.data[0].url\n",
        "    print(\"DALL-E image URL:\", image_url)\n",
        "    image_name_dalle =\"/content/drive/MyDrive/GenAI Image Generation/image/\"+f\"{prompt_item}_dalle_{prompt_item_num}.png\"\n",
        "    save_image_from_url(image_url, image_name_dalle)\n",
        "    return image_name_dalle"
      ],
      "metadata": {
        "id": "pnQOcjAm2sYe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agent function to create image based on product name, returns file path of generated Dalle image\n",
        "\n",
        "def agent_generate_image_product_dalle(product_name: str) -> str:\n",
        "\n",
        "  #Get prompt to generate image from product name using cosine similarity in vector db and chat-gpt 4o\n",
        "  prompt_text = get_image_prompt_for_product(product_name)\n",
        "\n",
        "  #Clean up the prompt for Dalle input\n",
        "  final_prompt = create_prompt_image_generation(prompt_text)\n",
        "\n",
        "  #Final prompt for Dalle with specific instructions\n",
        "  prompt_final_image = \"Generate high-resolution, detailed, realistic image of a \"+ product_name +\": \"+ final_prompt\n",
        "  print(prompt_final_image)\n",
        "\n",
        "  #Call function to generate image from Dalle with product name and final prompt, the last parameter is for suffix in image name, default suffix is 1 eg. Mesh_Chair_Dalle_1.png\n",
        "  image_path_dalle = generate_image_dalle(product_name, prompt_final_image,1)\n",
        "\n",
        "  #Return saved image path\n",
        "  return image_path_dalle\n"
      ],
      "metadata": {
        "id": "uXX7x5nd1-GL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent function to create image based on product name, returns file path of generated SD image\n",
        "\n",
        "def agent_generate_image_product_sd(product_name: str) -> str:\n",
        "\n",
        "    prompt_text = get_image_prompt_for_product(product_name)\n",
        "\n",
        "    final_prompt = create_prompt_image_generation(prompt_text)\n",
        "\n",
        "    prompt_final_image = \"Studio quality product photo, highly detailed, cinematic lighting of a \"+ product_name +\": \"+ final_prompt\n",
        "    print(prompt_final_image)\n",
        "\n",
        "    image_path_sd = generate_image_sd(product_name, prompt_final_image, 1)\n",
        "\n",
        "    # Return saved image path\n",
        "    return image_path_sd"
      ],
      "metadata": {
        "id": "BE9J4u3_bmiq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_sd = agent_generate_image_product_sd(\"mesh chair\")\n",
        "\n",
        "# Display the image\n",
        "image = Image.open(image_sd)\n",
        "width, height = image.size\n",
        "new_size = (width // 4, height // 4)\n",
        "small_image = image.resize(new_size, Image.LANCZOS)\n",
        "display(small_image)"
      ],
      "metadata": {
        "id": "ajKawVe1cIrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Call agent with product name to generate image\n",
        "image_dalle = agent_generate_image_product_dalle(\"mesh chair\")\n",
        "\n",
        "# Display the image\n",
        "image = Image.open(image_dalle)\n",
        "width, height = image.size\n",
        "new_size = (width // 4, height // 4)\n",
        "small_image = image.resize(new_size, Image.LANCZOS)\n",
        "display(small_image)"
      ],
      "metadata": {
        "id": "_QHypd643v43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the image\n",
        "image = Image.open(image_dalle)\n",
        "\n",
        "# Resize the image to 1/4th of its original size\n",
        "width, height = image.size\n",
        "new_size = (width // 4, height // 4)\n",
        "small_image = image.resize(new_size, Image.LANCZOS)\n",
        "\n",
        "# Display the smaller image\n",
        "display(small_image)"
      ],
      "metadata": {
        "id": "YaI1sdLX-gJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image.open(image_sd))"
      ],
      "metadata": {
        "id": "FeahFetqi285"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}